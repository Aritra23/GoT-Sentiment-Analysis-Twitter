{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lahir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from textblob import TextBlob\n",
    "from tweepy import Stream\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pprint\n",
    "import json\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "from sqlalchemy import func\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np  \n",
    "import re  \n",
    "import nltk  \n",
    "from sklearn.datasets import load_files  \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "nltk.download('stopwords')  \n",
    "import pickle  \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "# Define the 'classDB' database in Mongo\n",
    "db = client.projDB\n",
    "tweet = db.tweet_hashtag.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"character_lower\", \"character\", \"id\", \"text\", \"language\",\"created\", \"place\", \"source\", \"sentiment\", \"polarity\", \"subjectivity\", \"Preprocess\"])\n",
    "df1 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for each_tweet in tweet:\n",
    "#     print(each_tweet)\n",
    "#     print(\"\")\n",
    "#     print (each_tweet[\"id\"])\n",
    "#     print(each_tweet[\"text\"])\n",
    "    analysis = TextBlob(each_tweet[\"text\"])\n",
    "#     print(analysis.sentiment)\n",
    "    senti = \"\"\n",
    "    polarity = 0.0\n",
    "    subjectivity = 0.0\n",
    "    \n",
    "    if analysis.sentiment[0]>0:\n",
    "#        print(\"pos\")\n",
    "       senti = \"Pos\"\n",
    "    elif analysis.sentiment[0]<0:\n",
    "#        print(\"neg\")\n",
    "       senti = \"Neg\"\n",
    "    else:\n",
    "#        print(\"neu\")\n",
    "       senti = \"Neu\"\n",
    "    \n",
    "    polarity = float(analysis.sentiment[0])\n",
    "    subjectivity = float(analysis.sentiment[1])\n",
    "    \n",
    "#     name = each_tweet[\"entities\"]\n",
    "#     print(each_tweet[\"entities\"])\n",
    "#     print(senti)\n",
    "#     print(\"\")\n",
    "#     print(each_tweet[\"entities\"][\"hashtags\"][0][\"text\"])\n",
    "    if each_tweet[\"entities\"][\"hashtags\"] is None:\n",
    "        char_name = \"\"\n",
    "    else:\n",
    "        if len(each_tweet[\"entities\"][\"hashtags\"]) > 0:\n",
    "            char_name = each_tweet[\"entities\"][\"hashtags\"][0][\"text\"]\n",
    "    \n",
    "#     print(char_name)\n",
    "    try: \n",
    "        df1 = pd.DataFrame({\"character_lower\": char_name.lower(), \"character\": char_name, \"id\": each_tweet[\"id\"], \"text\": each_tweet[\"text\"], \"language\": each_tweet[\"language\"], \"created\": each_tweet[\"created\"], \"place\": each_tweet[\"place\"], \"source\": each_tweet[\"source\"], \"sentiment\": [senti], \"polarity\": [polarity], \"subjectivity\": [subjectivity], \"Preprocess\": \"\"})\n",
    "        df = df.append(df1, ignore_index=True)\n",
    "    except:\n",
    "#         print(each_tweet)\n",
    "#         print(f'{char_name}, {each_tweet[\"id\"]}, {each_tweet[\"text\"]}, {each_tweet[\"language\"]}, {each_tweet[\"created\"]}, {each_tweet[\"place\"]}, {each_tweet[\"source\"]}, {[senti]}')\n",
    "        pass\n",
    "    count += 1\n",
    "    if count > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backup = df\n",
    "# df_backup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character_lower</th>\n",
       "      <th>character</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>created</th>\n",
       "      <th>place</th>\n",
       "      <th>source</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>Preprocess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bladerunner</td>\n",
       "      <td>BladeRunner</td>\n",
       "      <td>1127826742507294721</td>\n",
       "      <td>Did this just turn into #BladeRunner ? #GamefT...</td>\n",
       "      <td>en</td>\n",
       "      <td>Mon May 13 06:43:44 +0000 2019</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>Neu</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bladerunner</td>\n",
       "      <td>BladeRunner</td>\n",
       "      <td>1127826754913984512</td>\n",
       "      <td>RT @barstoolsports: Arya Stark tomorrow mornin...</td>\n",
       "      <td>en</td>\n",
       "      <td>Mon May 13 06:43:47 +0000 2019</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>Neg</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gameofthrones</td>\n",
       "      <td>GameofThrones</td>\n",
       "      <td>1127826760136036354</td>\n",
       "      <td>RT @LagbhagSecular: Bran send Uber ride for Ar...</td>\n",
       "      <td>en</td>\n",
       "      <td>Mon May 13 06:43:48 +0000 2019</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>Neg</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gameofthrones</td>\n",
       "      <td>GameofThrones</td>\n",
       "      <td>1127826761855541248</td>\n",
       "      <td>RT @IamIamerna: We SALUTE to a legend, who ful...</td>\n",
       "      <td>en</td>\n",
       "      <td>Mon May 13 06:43:48 +0000 2019</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>Neg</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.477778</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gameofthrones</td>\n",
       "      <td>GameofThrones</td>\n",
       "      <td>1127826783674544128</td>\n",
       "      <td>RT @imthedopey: Ser Sandor \"The Hound\" Clegane...</td>\n",
       "      <td>en</td>\n",
       "      <td>Mon May 13 06:43:53 +0000 2019</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>Pos</td>\n",
       "      <td>0.211111</td>\n",
       "      <td>0.644444</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  character_lower      character                   id  \\\n",
       "0     bladerunner    BladeRunner  1127826742507294721   \n",
       "1     bladerunner    BladeRunner  1127826754913984512   \n",
       "2   gameofthrones  GameofThrones  1127826760136036354   \n",
       "3   gameofthrones  GameofThrones  1127826761855541248   \n",
       "4   gameofthrones  GameofThrones  1127826783674544128   \n",
       "\n",
       "                                                text language  \\\n",
       "0  Did this just turn into #BladeRunner ? #GamefT...       en   \n",
       "1  RT @barstoolsports: Arya Stark tomorrow mornin...       en   \n",
       "2  RT @LagbhagSecular: Bran send Uber ride for Ar...       en   \n",
       "3  RT @IamIamerna: We SALUTE to a legend, who ful...       en   \n",
       "4  RT @imthedopey: Ser Sandor \"The Hound\" Clegane...       en   \n",
       "\n",
       "                          created place  \\\n",
       "0  Mon May 13 06:43:44 +0000 2019  None   \n",
       "1  Mon May 13 06:43:47 +0000 2019  None   \n",
       "2  Mon May 13 06:43:48 +0000 2019  None   \n",
       "3  Mon May 13 06:43:48 +0000 2019  None   \n",
       "4  Mon May 13 06:43:53 +0000 2019  None   \n",
       "\n",
       "                                              source sentiment  polarity  \\\n",
       "0  <a href=\"http://twitter.com/download/iphone\" r...       Neu  0.000000   \n",
       "1  <a href=\"http://twitter.com/download/iphone\" r...       Neg -0.200000   \n",
       "2  <a href=\"http://twitter.com/download/iphone\" r...       Neg -0.200000   \n",
       "3  <a href=\"http://twitter.com/download/iphone\" r...       Neg -0.050000   \n",
       "4  <a href=\"http://twitter.com/download/android\" ...       Pos  0.211111   \n",
       "\n",
       "   subjectivity Preprocess  \n",
       "0      0.000000             \n",
       "1      0.600000             \n",
       "2      0.600000             \n",
       "3      0.477778             \n",
       "4      0.644444             "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did this just turn into #BladeRunner ? #GamefThrones #AryaStark https://t.co/CL1RKbrHCc\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\lahir/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\lahir\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\lahir\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\lahir\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\lahir\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\lahir/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\lahir\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\lahir\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\lahir\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\lahir\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-c692c0b73c18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mtest_preproc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_preproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mtest_preproc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_preproc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_preproc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mcomb_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_preproc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-c692c0b73c18>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mtest_preproc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_preproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mtest_preproc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_preproc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_preproc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mcomb_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_preproc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\stem\\wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\lahir/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\lahir\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\lahir\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\lahir\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\lahir\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # Remove all the special characters\n",
    "    print(row[\"text\"])\n",
    "    test_preproc = re.sub(r'\\W', ' ', str(row[\"text\"]))\n",
    "\n",
    "    # remove all single characters\n",
    "    test_preproc = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', test_preproc)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    test_preproc = re.sub(r'\\^[a-zA-Z]\\s+', ' ', test_preproc) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    test_preproc = re.sub(r'\\s+', ' ', test_preproc, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    test_preproc = re.sub(r'^b\\s+', '', test_preproc)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    test_preproc = test_preproc.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    test_preproc = test_preproc.split()\n",
    "\n",
    "    test_preproc = [stemmer.lemmatize(word) for word in test_preproc]\n",
    "    print(test_preproc)\n",
    "    comb_string = ' '.join(test_preproc)\n",
    "    row[\"Preprocess\"] = comb_string\n",
    "    print(comb_string)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character_lower</th>\n",
       "      <th>character</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>created</th>\n",
       "      <th>place</th>\n",
       "      <th>source</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>Preprocess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>got</td>\n",
       "      <td>GoT</td>\n",
       "      <td>1127826700736159744</td>\n",
       "      <td>RT @IamIamerna: We SALUTE to a legend, who ful...</td>\n",
       "      <td>en</td>\n",
       "      <td>Mon May 13 06:43:34 +0000 2019</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>Neg</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.477778</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bladerunner</td>\n",
       "      <td>BladeRunner</td>\n",
       "      <td>1127826742507294721</td>\n",
       "      <td>Did this just turn into #BladeRunner ? #GamefT...</td>\n",
       "      <td>en</td>\n",
       "      <td>Mon May 13 06:43:44 +0000 2019</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>Neu</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bladerunner</td>\n",
       "      <td>BladeRunner</td>\n",
       "      <td>1127826754913984512</td>\n",
       "      <td>RT @barstoolsports: Arya Stark tomorrow mornin...</td>\n",
       "      <td>en</td>\n",
       "      <td>Mon May 13 06:43:47 +0000 2019</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>Neg</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.600000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gameofthrones</td>\n",
       "      <td>GameofThrones</td>\n",
       "      <td>1127826760136036354</td>\n",
       "      <td>RT @LagbhagSecular: Bran send Uber ride for Ar...</td>\n",
       "      <td>en</td>\n",
       "      <td>Mon May 13 06:43:48 +0000 2019</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>Neg</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.600000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gameofthrones</td>\n",
       "      <td>GameofThrones</td>\n",
       "      <td>1127826761855541248</td>\n",
       "      <td>RT @IamIamerna: We SALUTE to a legend, who ful...</td>\n",
       "      <td>en</td>\n",
       "      <td>Mon May 13 06:43:48 +0000 2019</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>Neg</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.477778</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  character_lower      character                   id  \\\n",
       "0             got            GoT  1127826700736159744   \n",
       "1     bladerunner    BladeRunner  1127826742507294721   \n",
       "2     bladerunner    BladeRunner  1127826754913984512   \n",
       "3   gameofthrones  GameofThrones  1127826760136036354   \n",
       "4   gameofthrones  GameofThrones  1127826761855541248   \n",
       "\n",
       "                                                text language  \\\n",
       "0  RT @IamIamerna: We SALUTE to a legend, who ful...       en   \n",
       "1  Did this just turn into #BladeRunner ? #GamefT...       en   \n",
       "2  RT @barstoolsports: Arya Stark tomorrow mornin...       en   \n",
       "3  RT @LagbhagSecular: Bran send Uber ride for Ar...       en   \n",
       "4  RT @IamIamerna: We SALUTE to a legend, who ful...       en   \n",
       "\n",
       "                          created place  \\\n",
       "0  Mon May 13 06:43:34 +0000 2019  None   \n",
       "1  Mon May 13 06:43:44 +0000 2019  None   \n",
       "2  Mon May 13 06:43:47 +0000 2019  None   \n",
       "3  Mon May 13 06:43:48 +0000 2019  None   \n",
       "4  Mon May 13 06:43:48 +0000 2019  None   \n",
       "\n",
       "                                              source sentiment  polarity  \\\n",
       "0  <a href=\"http://twitter.com/download/iphone\" r...       Neg     -0.05   \n",
       "1  <a href=\"http://twitter.com/download/iphone\" r...       Neu      0.00   \n",
       "2  <a href=\"http://twitter.com/download/iphone\" r...       Neg     -0.20   \n",
       "3  <a href=\"http://twitter.com/download/iphone\" r...       Neg     -0.20   \n",
       "4  <a href=\"http://twitter.com/download/iphone\" r...       Neg     -0.05   \n",
       "\n",
       "   subjectivity Preprocess  \n",
       "0      0.477778             \n",
       "1      0.000000             \n",
       "2      0.600000             \n",
       "3      0.600000             \n",
       "4      0.477778             "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[\"character_lower\"].unique()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Remove all the special characters\n",
    "    string = re.sub(r'\\W', ' ', str(record[\"text\"]))\n",
    "\n",
    "    # remove all single characters\n",
    "    record[\"Preprocess\"] = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', record[\"Preprocess\"])\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    record[\"Preprocess\"] = re.sub(r'\\^[a-zA-Z]\\s+', ' ', record[\"Preprocess\"]) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    df[\"Preprocess\"] = re.sub(r'\\s+', ' ', df[\"Preprocess\"], flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    df[\"Preprocess\"] = re.sub(r'^b\\s+', '', df[\"Preprocess\"])\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    df[\"Preprocess\"] = df[\"Preprocess\"].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_textpreprocess_backup = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_textpreprocess_backup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-f4e3b0aef34f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_textpreprocess_backup\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sentiment\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtfidfconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_textpreprocess_backup' is not defined"
     ]
    }
   ],
   "source": [
    "df = df_textpreprocess_backup\n",
    "X = []\n",
    "y = df[\"sentiment\"]\n",
    "\n",
    "tfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
    "X = tfidfconverter.fit_transform(df[\"Preprocess\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=500, random_state=0)  \n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  \n",
    "print(accuracy_score(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
